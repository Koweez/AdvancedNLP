{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install human_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:58:53.927877Z","iopub.execute_input":"2024-12-12T12:58:53.928270Z","iopub.status.idle":"2024-12-12T12:59:06.753634Z","shell.execute_reply.started":"2024-12-12T12:58:53.928236Z","shell.execute_reply":"2024-12-12T12:59:06.752441Z"}},"outputs":[{"name":"stdout","text":"Collecting human_eval\n  Downloading human_eval-1.0.3-py3-none-any.whl.metadata (153 bytes)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from human_eval) (4.66.4)\nCollecting fire (from human_eval)\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from human_eval) (1.26.4)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->human_eval) (2.4.0)\nDownloading human_eval-1.0.3-py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=ba4039b8c5007bc722f7943e264a12b41e9433243848c1f3e62420a6370bbcd3\n  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\nSuccessfully built fire\nInstalling collected packages: fire, human_eval\nSuccessfully installed fire-0.7.0 human_eval-1.0.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport human_eval\nimport time\nimport requests\nimport json\n\nfrom human_eval.data import write_jsonl, read_problems\nfrom human_eval.evaluation import evaluate_functional_correctness\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:59:06.755698Z","iopub.execute_input":"2024-12-12T12:59:06.756004Z","iopub.status.idle":"2024-12-12T12:59:11.612232Z","shell.execute_reply.started":"2024-12-12T12:59:06.755957Z","shell.execute_reply":"2024-12-12T12:59:11.611516Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def benchmark_model(model, tokenizer, model_name, dataset_path=None):\n    if dataset_path is None:\n        dataset_path = os.path.join(\n            os.path.dirname(human_eval.__file__), \n            'data', \n            'HumanEval.jsonl.gz'\n        )\n        \n    problems = read_problems(dataset_path)\n    \n    solutions = []\n\n    num_generations_per_problem = 1  # Ensure enough generations for pass@1, pass@10 and pass@100\n\n    cumulative_time = 0\n    for task_id, problem in tqdm(problems.items()):\n        prompt = problem['prompt']\n        task_solutions = []\n        for _ in range(num_generations_per_problem):\n            inputs = tokenizer(prompt, return_tensors=\"pt\")\n            start = time.time()\n            generate_ids = model.generate(inputs.input_ids.cuda(), pad_token_id=tokenizer.eos_token_id, attention_mask=inputs[\"attention_mask\"].cuda())\n            cumulative_time += time.time() - start\n            solution = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n            task_solutions.append({\n                'task_id': task_id,\n                'prompt': prompt,\n                'completion': solution\n            })\n        \n        solutions.extend(task_solutions)\n\n    \n    output_path = f'{model_name}_humaneval_solutions.jsonl'\n    write_jsonl(output_path, solutions)\n    \n    results = evaluate_functional_correctness(\n        output_path, \n        n_workers=4,\n        timeout=3.0,\n        k=[1]\n    )\n    \n    return {\n        'model_name': model_name,\n        'pass_at_1': results['pass@1'],\n        'total_problems': len(problems),\n        'solutions_path': output_path,\n        'time': cumulative_time\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:59:11.613324Z","iopub.execute_input":"2024-12-12T12:59:11.613699Z","iopub.status.idle":"2024-12-12T12:59:11.621918Z","shell.execute_reply.started":"2024-12-12T12:59:11.613671Z","shell.execute_reply":"2024-12-12T12:59:11.620916Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"models = ['Qwen/Qwen2.5-Coder-3B', 'bigcode/starcoder2-3b', 'stabilityai/stable-code-3b', 'ibm-granite/granite-3b-code-base-2k', 'openbmb/MiniCPM3-4B']\nmodel_name = 'Qwen/Qwen2.5-Coder-3B'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).cuda()\nresults = benchmark_model(\n    model,\n    tokenizer,\n    model_name=model_name\n)\n\nprint(results)\nprint(json.dumps(results, indent=2))\n\nwith open(f'{results[\"model_name\"]}_benchmark_results.json', 'w') as f:\n    json.dump(results, f, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:59:11.623649Z","iopub.execute_input":"2024-12-12T12:59:11.623902Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a5af3c15b1f4fe792f9e9ec42aa2d14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"915a7af6de4345d490631ad9c0188bde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b6e051f93af441f93a5660d4bedf243"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eed960f9545f4d23baa6bad1a0dac2db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a9acabdcf194712a96bfda06ccc87b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb6faf91f2a748faa201314beda920d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"630fd39a6c4548bbb60ade5a80c78776"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a501d6db6f8e4126af9471ec8f40526e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.21G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"345732ba478d4d239088b83cd54043e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"192c31a049e546e2ac97dd47cfe0755b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/139 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9a5215579114e3ea2e4adcf56a1501a"}},"metadata":{}},{"name":"stderr","text":"  1%|          | 2/164 [01:39<2:36:04, 57.81s/it]","output_type":"stream"}],"execution_count":null}]}